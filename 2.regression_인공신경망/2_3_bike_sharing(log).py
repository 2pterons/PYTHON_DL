# -*- coding: utf-8 -*-
"""2-3.bike_sharing(log).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qXwpVFU62AGODklfhSHt9nz_Q-5BDIj1
"""

# 자전거 대여 횟수 (count) 예측
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/My Drive/Colab Notebooks'

df = pd.read_csv('data/bike_train.csv')
df.info()

sns.displot(df['count'])
plt.show()

# datetime을 년, 월, 일, 시간 컬럼으로 분리
df['datetime'] = df['datetime'].apply(pd.to_datetime)
df['year'] = df['datetime'].apply(lambda x: x.year)
df['month'] = df['datetime'].apply(lambda x: x.month)
df['day'] = df['datetime'].apply(lambda x: x.day)
df['hour'] = df['datetime'].apply(lambda x: x.hour)

# 불필요한 컬럼 삭제
df.drop(['datetime', 'casual', 'registered'], axis = 1, inplace = True)

# 학습 데이터
y_target = np.array(np.log1p(df['count'])).reshape(-1, 1)  # count가 작은 값에 몰려 있으므로 log 변환함.
x_features = df.drop(['count'], axis=1)

# 데이터 표준화 (Z-score)
feat = ['temp', 'atemp', 'humidity', 'windspeed']
x_features[feat] = StandardScaler().fit_transform(x_features[feat])

# categorical feature들은 one-hot encoding 한다.
x_features_ohe = pd.get_dummies(x_features, columns=['year', 'month', 'day', 'hour', 'holiday', 'workingday', 'season', 'weather'])
x_features_ohe = np.array(x_features_ohe)

# 학습 데이터와 시험 데이터로 분리
x_train, x_test, y_train, y_test = train_test_split(x_features_ohe, y_target,test_size = 0.2)
x_train.shape, x_test.shape

# target 값을 표준화한다.
scaleY = StandardScaler()
scaleY.fit(y_train)
y_train = scaleY.transform(y_train)
print('train data의 평균 = {:.2f}, 분산 = {:.2f}'.format(scaleY.mean_[0], scaleY.var_[0]))

# test target은 알고 있는 값이 아니므로 train target의 평균과 표준편차를 사용한다.
# train, test target의 분포는 유사하다는 가정이 전제되어 있다.
y_test = scaleY.transform(y_test)

# 인공신경망 모델을 생성한다.
n_feature = 73
n_hidden = 32
n_output = 1

x_input = Input(batch_shape = (None, n_feature))
h_layer = Dense(n_hidden, activation='relu')(x_input)
h_layer = Dropout(rate = 0.5)(h_layer)
h_layer = Dense(n_hidden, activation='relu')(h_layer)
h_layer = Dropout(rate = 0.5)(h_layer)
y_output = Dense(n_output)(h_layer)

model = Model(x_input, y_output)
model.compile(loss='mse', optimizer=Adam(learning_rate = 0.001))
model.summary()

# 학습
hist = model.fit(x_train, y_train, batch_size=64, epochs=100, validation_data = (x_test, y_test))

plt.plot(hist.history['loss'], label='train')
plt.plot(hist.history['val_loss'], label='test')
plt.legend()
plt.show()

# x_test의 결과를 추정한다.
y_pred = model.predict(x_test)

# y_pred 값을 원래의 값으로 복원한다. (log 변환 후 표준화 했었음)
y_pred_org = np.expm1(scaleY.inverse_transform(y_pred))

# y_test 값도 원래의 값으로 복원한다.
y_test_org = np.expm1(scaleY.inverse_transform(y_test))

# 추정 결과를 육안으로 비교해 본다.
y_df = pd.DataFrame({'y_test':y_test_org.reshape(-1), 'y_pred':y_pred_org.reshape(-1)})
y_df.head(50)

# R2 score를 측정한다. (log, 표준화된 상태에서 측정)
print('Log, 표준화 변환 상태 :')
print('RMSE = {:.4f}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))
print(' R^2 = {:.4f}'.format(r2_score(y_test, y_pred)))

# 원래 데이터로 복원한 상태에서 측정
print('\n원래 데이터로 복원한 상태 :')
print('RMSE = {:.4f}'.format(np.sqrt(mean_squared_error(y_test_org, y_pred_org))))
print(' R^2 = {:.4f}'.format(r2_score(y_test_org, y_pred_org)))

sns.kdeplot(y_test.reshape(-1))
sns.kdeplot(y_pred.reshape(-1))
plt.show()

sns.kdeplot(y_test_org.reshape(-1))
sns.kdeplot(y_pred_org.reshape(-1))
plt.show()

