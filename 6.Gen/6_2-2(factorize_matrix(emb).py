# -*- coding: utf-8 -*-
"""6/29 화-수업(Factorize_matrix(Emb).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FKkyjlmDS68Kts7QB_mXleyEYMLEnpXG
"""

# -*- coding: utf-8 -*-
"""6-2.factorize_matrix(Emb).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nBnaostrUb3295DO7zV82Jc2grjCkuC4
"""

# 행렬 분해 : R = P * Q.T
# NaN이 포함된 R이 주어졌을 때 P, Q를 추정한다.
# by Embedding layers
# -------------------------------------------
import numpy as np
import pandas as pd
from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Flatten, Dot, Activation
import tensorflow.keras.backend as K # layers에 없는 걸 k값으로 써라
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# User-item matrix
N = np.NaN
R = np.array([[4, N, N, 2, N],
              [N, 5, N, 3, 1],
              [N, N, 3, 4, 4],
              [5, 2, 1, 2, N]])
R = np.array(R)

n_users = R.shape[0]
n_items = R.shape[1]
n_factors = 3

# unpivoting
user_item = pd.DataFrame(R).stack().reset_index()
user_item.columns = ['user', 'item', 'rating']

x_user = np.array(user_item['user']).reshape(-1, 1)
x_item = np.array(user_item['item']).reshape(-1, 1)
y_rating = np.array(user_item['rating']).reshape(-1, 1)
x_user.shape, x_item.shape

def my_activation(x):
  #return K.sigmoid(x)
  return K.hard_sigmoid(x)
  #return K.clip(x, 0, 1) # 0 이하는 0으로 1이상은 1로


# hard_sigmoid activation function
x_input_user = Input(batch_shape = (None, x_user.shape[1]))
x_input_item = Input(batch_shape = (None, x_item.shape[1]))

x_user_emb = Embedding(input_dim = n_users, output_dim = n_factors)(x_input_user)
# 갯수를 지정해 줘야한다. categorical n_users와 output n_factos를 정해줘야한다.
# 내부에서 원-핫으로 바꿔준다. 
# !!매우중요!!Embedding layer : 나눠서 하기때문에 메모리 문제가 해결되고, 내부적으로 행렬연산을 하지않고 "look up process"로 처리한다.
# 특징1: to_categorical 로 변환해준다
# 특징2: 내부적으로 행렬연산을 하지않고 "look up process"로 처리한다.
# 특징3: 2D로 입력을 받아서 3D로 출력을 받는다
x_user_emb = Flatten()(x_user_emb)
# 3D를 2D로 바꿔주기 위해 Flatten을 써 주었다. 

x_item_emb = Embedding(input_dim = n_items, output_dim = n_factors)(x_input_item)
x_item_emb = Flatten()(x_item_emb)

y_output = Dot(axes=1)([x_user_emb, x_item_emb])
# y_output = Activation('sigmoid')(y_output) <-- 0~1
y_output = Activation(my_activation)(y_output) # = Activation('sigmoid')


# sigmoid를 써도 되는지 검토해본다.(계산비용이 크다)
# clip을 통해 써도 되는지 안되는지 찾아본다.
model = Model([x_input_user, x_input_item], y_output)
model.compile(loss='mse', optimizer = Adam(learning_rate=0.01))
# 음수값이 나오면 값이 이상해 질 수 있으니간  0~1사이로 바꿔줘야한다.
# simoid를 쓰면 classification(binary_crossentorpy)문제로 가려는 경향이 있지만, 그려봤을때 나눠지는 현상이 없으면 써도 된다.
# 일반적으로는 쓰면 안된다. 

model_p = Model([x_input_user, x_input_item], x_user_emb)
model_q = Model([x_input_user, x_input_item], x_item_emb)

model.summary()

hist = model.fit([x_user, x_item], y_rating, epochs = 500)

y_pred = model.predict([x_user, x_item])

user_item['y_pred'] = y_pred
user_item

# user-item의 전체 조합을 생성한다
users = np.arange(n_users)
items = np.arange(n_items)

x_tot = np.array([(x, y) for x in users for y in items])
x_tot_user = x_tot[:, 0].reshape(-1, 1)
x_tot_item = x_tot[:, 1].reshape(-1, 1)

# user-item의 전체 조합에 대해 expected rating을 추정한다.
y_pred = model.predict([x_tot_user, x_tot_item])

df = pd.DataFrame([x_tot_user.reshape(-1), x_tot_item.reshape(-1), y_pred.reshape(-1)]).T
df.columns = ['user', 'item', 'rating']

ER = np.array(df.pivot_table('rating', index='user', columns='item')) * 5.0

ER.round(2)

R

P = model_p.predict([x_tot_user, x_tot_item])
Q = model_q.predict([x_tot_user, x_tot_item])

P.round(2)

Q.T.round(2)

import seaborn as sns

sns.displot(ER.reshape(20))

