# -*- coding: utf-8 -*-
"""1-1.이차방정식(tensorflow_1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dd9m-tDn2qEP9PyMiUyEX4NyTUCCSArV
"""

# Tensorflow 버전 : 직접 미분한 방법을 사용한 예시
# x, y 데이터 세트가 있을 때, 이차 방정식 y = w1x^2 + w2x + b를 만족하는
# parameter w1, w2, b를 추정한다.
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# y = 2x^2 + 3x + 5 일 때 x, y 데이터 집합을 생성한다
x = np.array(np.arange(-5, 5, 0.1))
y = 2 * x * x + 3 * x + 5

# x, y 만족하는 w1, w2, b를 찾는다.
# y = w1x^2 + w2x + b -->  w1 = 2, w2 = 3, b = 5가 나와야 한다.
lr = 0.01   # learning rate

# 그래프를 생성한다.   
w1 = tf.Variable(1.0)
w2 = tf.Variable(1.0)
b = tf.Variable(1.0)

histLoss = []
for epoch in range(1000):
    with tf.GradientTape() as tape:
        # loss 함수 : root mean squared error
        loss = tf.sqrt(tf.reduce_mean(tf.square(w1 * x * x + w2 * x + b - y)))
        
        # loss에 대한 각 variable들의 미분값을 계산한다.
        dw1, dw2, db = tape.gradient(loss, [w1, w2, b])
        
    # variable들을 업데이트한다 (Gradient descent)
    w1.assign_sub(lr * dw1)     # w1 <- w1 - lr * dw1의 의미
    w2.assign_sub(lr * dw2)     # w2.assign(w2 - lr * dw2.numpy())와 동일함
    b.assign_sub(lr * db)
    
    if epoch % 10 == 0:
        histLoss.append(loss.numpy())
        print("epoch = %d, loss = %.4f" % (epoch, loss.numpy()))

print("\n추정 결과 :")
print("w1 = %.2f" % w1.numpy())
print("w2 = %.2f" % w2.numpy())
print("b = %.2f" % b.numpy())
print("final loss = %.4f" % loss.numpy())

plt.plot(histLoss, color='red', linewidth=1)
plt.title("Loss function")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()