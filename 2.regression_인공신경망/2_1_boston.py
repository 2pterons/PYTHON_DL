# -*- coding: utf-8 -*-
"""2-1.boston.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16GQZRonMiRaez7ObDANcrxNmt7VI9Qas
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Activation
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.regularizers import l1, l2, l1_l2
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error

# 데이터를 읽어온다.
boston = load_boston()

# 데이터를 표준화한다.
scaleX = StandardScaler()
scaleY = StandardScaler()

feature_data = scaleX.fit_transform(boston.data)
target_data = scaleY.fit_transform(boston.target.reshape(-1, 1))

# 학습 데이터와 시험 데이터로 분리한다.
x_train,x_test,y_train,y_test = train_test_split(feature_data, target_data, test_size=0.2)
x_train.shape, y_train.shape

x_input = Input(batch_shape = (None, 13))
h_layer = Dense(256, activation='relu')(x_input)
h_layer = Dropout(rate=0.3)(h_layer)
h_layer = Dense(256, activation='relu', kernel_regularizer=l2(0.00))(h_layer)
h_layer = Dense(256)(h_layer)
h_layer = BatchNormalization()(h_layer)
h_layer = Activation('relu')(h_layer)
y_output = Dense(1)(h_layer)
model = Model(x_input, y_output)
model.compile(loss='mse', optimizer=Adam(learning_rate=0.5))
model.summary()

hist = model.fit(x_train, y_train, batch_size = 10, epochs=50, validation_data = (x_test, y_test))

y_pred = model.predict(x_test)
print('MSE =', mean_squared_error(y_test, y_pred))

# 표준화된 y_pred를 원래값으로 역변환
y_pred = scaleY.inverse_transform(y_pred)
print(y_pred)

plt.plot(hist.history['loss'], label='train')
plt.plot(hist.history['val_loss'], label='test')
plt.legend()
plt.show()

